{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKm40E3h4bEVTAdWlvlryn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QD_vosy80jOF"},"outputs":[],"source":["!pip install torch tensorboard\n","!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes\n","\n","#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n","!pip install -U flash-attn --no-build-isolation\n","\n","!pip install peft --quiet\n","!pip install datasets trl ninja packaging\n","!pip install diffusers safetensors  --quiet\n","!pip install colab-env --quiet"]},{"cell_type":"code","source":["#### Hugging Face Token\n","\n","import os\n","\n","access_token = \"xxxx\"\n","access_token_write = \"xxxx\""],"metadata":{"id":"6UvqE3SS0uAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import login\n","\n","login(\n","  token=access_token_write,\n","  add_to_git_credential=True\n",")"],"metadata":{"id":"33zuBjzT0yCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","import sys\n","import json\n","import IPython\n","from datetime import datetime\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")\n","from trl import SFTTrainer"],"metadata":{"id":"2PpLqXv400V_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set device\n","device = 'cuda'"],"metadata":{"id":"FNvtpsDC03p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","import torch"],"metadata":{"id":"5PWJbREn06Jn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.__version__"],"metadata":{"id":"UmY1NvNL08Sj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt-get update && apt-get install -y cuda-11.8"],"metadata":{"id":"ScLMFxbX0_ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version\n","!nvcc --version\n","!nvidia-smi"],"metadata":{"id":"3s-Jbplb1B7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.__version__"],"metadata":{"id":"zph7yzCv1EQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","import sys\n","import json\n","import IPython\n","from datetime import datetime\n","from datasets import load_dataset\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")\n","from trl import SFTTrainer"],"metadata":{"id":"0OQmGOeD1HJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### conversational format\n","{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n","\n","### instruction format\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"],"metadata":{"id":"ZaLICdVc1KgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert dataset to OAI messages\n","system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","{schema}\"\"\""],"metadata":{"id":"z0IdZHl-1OIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_conversation(sample):\n","  return {\n","    \"messages\": [\n","      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n","      {\"role\": \"user\", \"content\": sample[\"question\"]},\n","      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n","    ]\n","  }"],"metadata":{"id":"XSeOKRsc1R5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset from the hub\n","dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n","dataset = dataset.shuffle().select(range(12500))"],"metadata":{"id":"WcyKi_K61VM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert dataset to OAI messages\n","dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)"],"metadata":{"id":"74x990EE1Y6e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split dataset into 10,000 training samples and 2,500 test samples\n","dataset = dataset.train_test_split(test_size=2500/12500)"],"metadata":{"id":"q2s8Sto51bIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset[\"train\"][345][\"messages\"])"],"metadata":{"id":"jprJbhxt1d1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save datasets to disk\n","dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n","dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"],"metadata":{"id":"EpTvf3ST1ghL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load jsonl data from disk for sql\n","dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"],"metadata":{"id":"gxz2IwJc1jN-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset[345][\"messages\"])"],"metadata":{"id":"NIaB0UXk1moh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from trl import setup_chat_format"],"metadata":{"id":"PW-eonEr1pRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hugging Face model id\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","# BitsAndBytesConfig int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",")"],"metadata":{"id":"QJ6cf5yW1sIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    attn_implementation=\"flash_attention_2\",\n","    torch_dtype=torch.bfloat16,\n","    quantization_config=bnb_config\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n","tokenizer.padding_side = 'right' # to prevent warnings"],"metadata":{"id":"NLylpnv01vnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n","tokenizer.pad_token = tokenizer.unk_token\n","tokenizer.pad_token_id = tokenizer.unk_token_id"],"metadata":{"id":"hwDh-1i71y1Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"B059wjKK11OF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import LoraConfig\n","\n","# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n","peft_config = LoraConfig(\n","        lora_alpha=128,\n","        lora_dropout=0.05,\n","        r=256,\n","        bias=\"none\",\n","        target_modules=\"all-linear\",\n","        task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"XbkNXoY-139t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install transformers==4.36.2 --quiet\n","#!pip install transformers accelerate --quiet"],"metadata":{"id":"pIkvuB1817Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments"],"metadata":{"id":"y6pkCeJd1-fA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir=\"Mistral-7B-text-to-sql-flash-attention-v20\",    # directory to save and repository id\n","    num_train_epochs=3,                     # number of training epochs\n","    per_device_train_batch_size=3,          # batch size per device during training\n","    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    logging_steps=10,                       # log every 10 steps\n","    save_strategy=\"epoch\",                  # save checkpoint every epoch\n","    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n","    bf16=True,                              # use bfloat16 precision\n","    tf32=True,                              # use tf32 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    push_to_hub=True,                       # push model to hub\n","    report_to=\"tensorboard\",                # report metrics to tensorboard\n",")"],"metadata":{"id":"8VIwgDW_2BAt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from trl import SFTTrainer\n","max_seq_length = 3072 # max sequence length for model and packing of the dataset\n"],"metadata":{"id":"ZcDnF8KK2Ere"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")"],"metadata":{"id":"oiZERMRV2H0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start training, the model will be automatically saved to the hub and the output directory\n","trainer.train()\n","\n","# save model\n","trainer.save_model()"],"metadata":{"id":"6xAZuuyu2LO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# free the memory again\n","# del model\n","# del trainer\n","torch.cuda.empty_cache()"],"metadata":{"id":"QhGISJui2OKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline\n","%cd /content/\n","peft_model_id = \"./Mistral-7B-text-to-sql-flash-attention-v20\""],"metadata":{"id":"3_nUi4nx2Q0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Model with PEFT adapter\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","  peft_model_id,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"Bo4EsI0W2UGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"ePsYSi4E2WzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from random import randint"],"metadata":{"id":"jXcaqe6C2bZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load our test dataset\n","eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n","rand_idx = randint(0, len(eval_dataset))"],"metadata":{"id":"CyNL9J4M2b-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test on sample\n","prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)"],"metadata":{"id":"uddCjZDH2fJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n","print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n","print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"],"metadata":{"id":"0FuiUvDT2jjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"9Quk0YkI2piG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(sample):\n","    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n","    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n","        return 1\n","    else:\n","        return 0"],"metadata":{"id":"ocBZNeIG2qTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["success_rate = []\n","number_of_eval_samples = 1000\n","# iterate over eval dataset and predict\n","for s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n","    success_rate.append(evaluate(s))\n","\n","# compute accuracy\n","accuracy = sum(success_rate)/len(success_rate)"],"metadata":{"id":"uAdMZ2rQ2tnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Accuracy: {accuracy*100:.2f}%\")"],"metadata":{"id":"TDCTLXE62xA2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline"],"metadata":{"id":"-IGkHLsj20qJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_model_id = \"iomegak12/Mistral-7B-text-to-sql-flash-attention-v20\"\n","\n","model = AutoPeftModelForCausalLM.from_pretrained( peft_model_id, device_map=\"auto\", torch_dtype=torch.float16 )\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"Z__Gz7Wh23XZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt='What was the first album Beyonc√© released as a solo artist?'\n","prompt = f\"Instruct: generate a SQL query.\\n{prompt}\\nOutput:\\n\" # for dataset b-mc2/sql-create-context\n","outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.9, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)"],"metadata":{"id":"dzbTPtYc268x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Question: %s'%prompt)\n","print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"],"metadata":{"id":"jgn5aGyV3CbH"},"execution_count":null,"outputs":[]}]}